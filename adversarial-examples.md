# ICLR2015
- Explaining and Harnessing Adversarial Examples
## Workshop
-  Towards Deep Neural Network Architectures Robust to Adversarial Examples

# ICLR2016
- Adversarial Manipulation of Deep Representations, [Code](https://github.com/fartashf/under_convnet)

# ICLR2017
-  On Detecting Adversarial Perturbations 
- Delving into Transferable Adversarial Examples and Black-box Attacks 
- Adversarial Machine Learning at Scale 
- A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Samples 
- Adversarial examples in the physical world
## Workshop
- Robustness to Adversarial Examples through an Ensemble of Specialists
- Adversarial Attacks on Neural Network Policies
- Tactics of Adversarial Attack on Deep Reinforcement Learning Agents
- DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples
- Delving into adversarial attacks on deep policies
- Adversarial Examples for Semantic Image Segmentation
- Early Methods for Detecting Adversarial Images
## Reject
- Adversarial examples for generative models
- Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models
- Simple Black-Box Adversarial Perturbations for Deep Networks 
- Adversarial examples for generative models

# ICLR2018
- Certifying Some Distributional Robustness with Principled Adversarial Training
- Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality
- Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models
- Stochastic Activation Pruning for Robust Adversarial Defense
- Thermometer Encoding: One Hot Way To Resist Adversarial Examples
- PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples
- Synthetic and Natural Noise Both Break Neural Machine Translation
- Multi-Scale Dense Networks for Resource Efficient Image Classification
- Certifying Some Distributional Robustness with Principled Adversarial Training
- Towards better understanding of gradient-based attribution methods for Deep Neural Networks 
- Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach 
- Towards Deep Learning Models Resistant to Adversarial Attacks
- Certified Defenses against Adversarial Examples
- Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models
- Ensemble Adversarial Training: Attacks and Defenses
- Attacking Binarized Neural Networks
- Noisy Networks For Exploration
- Understanding Deep Neural Networks with Rectified Linear Units
- Mitigating Adversarial Effects Through Randomization
- Decision Boundary Analysis of Adversarial Examples
- Generating Natural Adversarial Examples, [Code](https://github.com/zhengliz/natural-adversary)
- Spatially Transformed Adversarial Examples
- Robustness of Classifiers to Universal Perturbations: A Geometric Perspective 
- Fix your classifier: the marginal value of training the last weight layer 
- Countering Adversarial Images using Input Transformations 
## Workshop
- Adversarial Spheres
- Intriguing Properties of Adversarial Examples
- Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms
- Attacking the Madry Defense Model with $L_1$ based Adversarial Examples
- Black-box Attacks on Deep Neural Networks via Gradient Estimation
- On the Limitation of Local Intrinsic Dimensionality for Characterizing the Subspaces of Adversarial Examples
## Reject
- Synthesizing Robust Adversarial Examples
- Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training
- Ground-Truth Adversarial Examples
- Generating Adversarial Examples with Adversarial Networks
- Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks
- Enhancing the Transferability of Adversarial Examples with Noise Reduced Gradient
- Machine vs Machine: Minimax-Optimal Defense Against Adversarial Examples
- The Manifold Assumption and Defenses Against Adversarial Perturbations
- Adversarial Examples for Natural Language Classification Problems
- LatentPoison -- Adversarial Attacks On The Latent Space
- Towards Safe Deep Learning: Unsupervised Defense Against Generic Adversarial Attacks
- Exploring the Space of Black-box Attacks on Deep Neural Networks
- Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier
- Key Protected Classification for GAN Attack Resilient Collaborative Learning
- Clipping Free Attacks Against Neural Networks
- Grouping-By-ID: Guarding Against Adversarial Domain Shifts

# ICML2017
- Parseval Networks: Improving Robustness to Adversarial Examples

# ICML2018
- Analyzing the Robustness of Nearest Neighbors to Adversarial Examples
- Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope
- Adversarial Risk and the Dangers of Evaluating Against Weak Attacks
- Geometry Score: A Method For Comparing Generative Adversarial Networks
- LaVAN: Localized and Visible Adversarial Noise
- Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples
- Adversarial Attack on Graph Structured Data
- Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training
- Synthesizing Robust Adversarial Examples
- Adversarial Attacks under Restricted Threat Models

# CVPR2017
- Universal Adversarial Perturbations
- Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach
- Noise Robust Depth From Focus Using a Ring Difference Filter

# ICCV2017
- SafetyNet: Detecting and Rejecting Adversarial Examples Robustly
- Adversarial Examples for Semantic Segmentation and Object Detection
- Adversarial Image Perturbation for Privacy Protection -- A Game Theory Perspective
- Universal Adversarial Perturbations Against Semantic Image Segmentation
- Adversarial Examples Detection in Deep Networks With Convolutional Filter Statistics

# CVPR 2018
- Art of singular vectors and universal adversarial perturbations
- Deflecting Adversarial Attacks with Pixel Deflection
- Boosting Adversarial Attacks With Momentum
- On the Robustness of Semantic Segmentation Models to Adversarial Attacks
- Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser
- Defense Against Universal Adversarial Perturbations
- Generative Adversarial Perturbations
- Robust Physical-World Attacks on Deep Learning Visual Classification

# Useful Package
- [foolbox](https://github.com/bethgelab/foolbox)
- [Adversarial-Examples-in-PyTorch](https://github.com/akshaychawla/Adversarial-Examples-in-PyTorch)

# Other
- Adversarial Patch
- Practical Black-Box Attacks against Machine Learning
- Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks
- Adversarial Attacks Against Medical Deep Learning Systems